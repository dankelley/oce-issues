---
title: Can we speed up interpBarnes()?
author: Dan Kelley
---

How to work here

1. Remove oce and reinstall from CRAN.
2. Edit 1880a.R, setting build<-TRUE in the first line.
3. Uninstall the local oce, and install the CRAN version instead.
3. Type 'make' to create the 1880a_cran.rda file. (Note of the execution time.)
4. Add this rda file to the repo.
5. Edit 1880a.R again, changing the first line to build<-FALSE
6. Rebuild a local copy of oce.
7. Run 'make' again.  This should run without errors, unless the local oce is
   creating different results from the CRAN oce.


# Performance/coding notes

## Should we use C++ array-based methods more?

Instead of looping in the C manner,
```
for (int i = 0; i < nxg; i++)
   for (int j = 0; j < nyg; j++)
     zg(i, j) = zz(i, j);
```
we can use C++ style
```
zg = clone(zz);
```
which may be clearer to readers.  A similar thing can be done for updating
`z_last`.  The following are tests with 10 trials on a unloaded machine (no
browsers, no zoom, no music, no Rstudio, no skim, no GH desktop, no preview, no
skim, no zotero) using
`sh
./run.sh ; cat run.out ; Rscript ttest.R` with exp().

* index-style: 3.782 ± 0.042
* clone-style: 3.805 ± 0.053

The results are indistinguishable, within uncertainties.

**Decision.** I will leave the code with index-style (which it has had for
several years) but I have added a macro called `USE-CLONE` with which the clone
style can be used, instead.

## Use an alternate exp() calculator?

For years, `src/interp_barnes.cpp` has had remnant code to do a polynomial
approximation to exponential.  I had used this in previous works, gaining speed
increases by 3X, but had judged that this was an insufficient reason to change
the method, because then any author would have to say they were using a Barnes
method modified in such-and-such a way, inviting problems with reviewers.

That old code caused an error of under 0.1%, on some decades-old machine using
decades-old software, so I decided to revisit the idea, but testing the
accuracy more rigorously.

The idea is to write $exp(x) = exp(xi)*exp(xf)$, where $xi$ is the integer part
of $a$ and $xf=x-xi$. Then, use a lookup table for $exp(xi)$, and a Taylor
series for $exp(xf)$, which could perhaps be short because $xf$ is bounded
between 0 and 1.

Here is my test for 5-th order (and, just graphs, for order 6 and 7).

```{r}
# For the Barnes algorithm, we have exp(-distance^2), which is always exp(x)
# where x <= 0.
#
# Idea: write e.g.  exp(-5.1) as exp(-6) * exp(0.9),
# and use lookup table for the first part, for maybe -50 to 0 or
# whatever, and then make a lookup table or Taylor series (as below)
# for the fraction part.

# Integer part
xi <- seq(0, 50, 1)
ei <- exp(-xi)

# Taylor series
taylor <- function(x)
{
    1 + x * (1 + x * (1/2 + x * (1/6 + x * (1/24 + x/120))))
}

# Combine
EXP <- function(x)
{
    i <- as.integer(floor(x))
    f <- x - i
    I <- ei[1-i]
    F <- taylor(f)
    I * F
}

# Test
x <- seq(-20, 0, length.out=1000)
E <- unlist(lapply(x, EXP))
plot(x, 100*(1-E/exp(x)), type="o", pch=20, cex=0.5, ylab="Percent error")
mpe <- max(abs(100*(1-E/exp(x))))
mtext(sprintf("5th order Taylor series: max abs percent error=%.6f", mpe))
```


## A 6th order test

```{r echo=FALSE}
# For the Barnes algorithm, we have exp(-distance^2), which is always exp(x)
# where x <= 0.
#
# Idea: write e.g.  exp(-5.1) as exp(-6) * exp(0.9),
# and use lookup table for the first part, for maybe -50 to 0 or
# whatever, and then make a lookup table or Taylor series (as below)
# for the fraction part.

# Integer part
xi <- seq(0, 50, 1)
ei <- exp(-xi)

# Taylor series
taylor <- function(x)
{
    1 + x * (1 + x * (1/2 + x * (1/6 + x * (1/24 + x*(1/120 + x/720)))))
}

# Combine
EXP <- function(x)
{
    i <- as.integer(floor(x))
    f <- x - i
    I <- ei[1-i]
    F <- taylor(f)
    #message("i=", i, " I=",I, " f=", f, " F=", F)
    I * F
}

# Test
x <- seq(-20, 0, length.out=1000)
E <- unlist(lapply(x, EXP))
plot(x, 100*(1-E/exp(x)), type="o", pch=20, cex=0.5, ylab="Percent error")
mpe <- max(abs(100*(1-E/exp(x))))
mtext(sprintf("6th order Taylor series: max abs percent error=%.6f", mpe))
```

### 7-th order
```{r echo=FALSE}
# For the Barnes algorithm, we have exp(-distance^2), which is always exp(x)
# where x <= 0.
#
# Idea: write e.g.  exp(-5.1) as exp(-6) * exp(0.9),
# and use lookup table for the first part, for maybe -50 to 0 or
# whatever, and then make a lookup table or Taylor series (as below)
# for the fraction part.

# Integer part
xi <- seq(0, 50, 1)
ei <- exp(-xi)

# Taylor series
taylor <- function(x)
{
    1 + x * (1 + x * (1/2 + x * (1/6 + x * (1/24 + x*(1/120 + x*(1/720+x/5040))))))
}

# Combine
EXP <- function(x)
{
    i <- as.integer(floor(x))
    f <- x - i
    I <- ei[1-i]
    F <- taylor(f)
    #message("i=", i, " I=",I, " f=", f, " F=", F)
    I * F
}

# Test
x <- seq(-20, 0, length.out=1000)
E <- unlist(lapply(x, EXP))
plot(x, 100*(1-E/exp(x)), type="o", pch=20, cex=0.5, ylab="Percent error")
mpe <- max(abs(100*(1-E/exp(x))))
mtext(sprintf("7th order Taylor series: max abs percent error=%.6f", mpe))
```




## Q can we speed up fastexp() by using a zero cutoff at low end?

I wonder whether branch-prediction might cause the exp() to be called and
completed, providing a bottleneck.  (This seems unlikely, but makes for a time
test that is worth doing.)

**Conclusion.** We should retain the exp() call, because switching the low end
to zero does not speed things up, within uncertainties.

**Data.**

*  with zero cutoff:
```
PENDING
```
I  without zero cutoff:
```
```

# Q: how do speed and accuracy of fastexp() vary with order?

Test with 1e-20 < x < 0 (in README.Rmd). Below, MPE=max percent error, and time
is as reported by `sh ./run.sh ; cat run.out ; Rscript ttest.R`

The results are very confusing. Sometimes the fastexp() method is faster, and
other times it is slower. Below (set 2) represents what I think are the most
reliable tests, with no launching of apps between rebuilds.  I conclude that
fastexp() method is not advisable.  Sometimes, it is faster (but only 25% in my
tests) and sometimes it is slower.  Always, it is less accurate.  I suspect the
inconsistent speed results are because of different machine load in sets 1 and
2, owing to my closing multiple apps for set 2, but a loaded machine is not
unusual and, besides, I would want to have a lot more than 25% increase in
speed, to use a method that differs from published algorithms.

```
 Set Order MPE       time (10 trials, with 95% CI)

 1    5     0.059112 4.275 ± 0.082
 1    6     0.008273 4.250 ± 0.080
 1    7     0.001018 4.494 ± 0.170
 1    -     0        5.515 ± 0.023
 2    5     0.059112 4.128 ± 0.086
 2    5     0.059112 4.141 ± 0.114
 2    -     0        3.823 ± 0.063
 2    -     0        3.849 ± 0.050
```

